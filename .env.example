# Dataset Tracker Configuration
# ==============================
# Controls how datasets (alignments, trees) are stored and tracked
# Options: FileDatasetTracker, SQLiteDatasetTracker
# - FileDatasetTracker: Simple file-based storage (good for development)
# - SQLiteDatasetTracker: Database storage with better querying (recommended for production)
DATASET_TRACKER_TYPE=SQLiteDatasetTracker

# Directory where uploaded dataset files are stored
DATASET_LOCATION=/data/uploads

# Path to SQLite database for dataset metadata (only used with SQLiteDatasetTracker)
DATASET_TRACKER_DB_PATH=/data/stores/datasets.db

# Job Tracker Configuration
# =========================
# Controls how analysis jobs are tracked and monitored
# Options: FileJobTracker, SQLiteJobTracker, InMemoryJobTracker, RedisJobTracker
# - FileJobTracker: Simple file-based storage (good for development)
# - SQLiteJobTracker: Database storage with filtering/querying (recommended for production)
# - InMemoryJobTracker: Temporary storage, lost on restart (testing only)
# - RedisJobTracker: Distributed storage with Redis (for multi-instance deployments)
JOB_TRACKER_TYPE=SQLiteJobTracker

# Directory where job tracker stores its data
JOB_TRACKER_LOCATION=/data/stores

# Path to SQLite database for job metadata (only used with SQLiteJobTracker)
JOB_TRACKER_DB_PATH=/data/stores/jobs.db

# Scheduler Configuration
# =======================
# Controls how jobs are submitted to the compute cluster
# SLURM_INTERFACE: 'rest' for REST API mode or 'cli' for CLI mode
SLURM_INTERFACE=rest

# REST API Configuration (used when SLURM_INTERFACE=rest)
# -------------------------------------------------------
# When using REST mode, set SCHEDULER_TYPE=SlurmRestScheduler
SCHEDULER_TYPE=SlurmRestScheduler

# Base URL for the Slurm REST API
SLURM_REST_URL=http://c2:9200

# API path for job status queries
SLURM_REST_API_PATH=/slurmdb/v0.0.37

# API path for job submission (can differ from status path)
SLURM_REST_SUBMIT_API_PATH=/slurm/v0.0.37

# Name of the Slurm queue/partition to submit jobs to
SLURM_QUEUE_NAME=normal

# CLI Configuration (used when SLURM_INTERFACE=cli)
# -------------------------------------------------
# When using CLI mode, set SCHEDULER_TYPE=SlurmScheduler
# SCHEDULER_TYPE=SlurmScheduler

# Path to the service-slurm repository (for CLI mode)
# SLURM_SERVICE_PATH=../service-slurm

# Service Configuration
# =====================
# Port for the REST API service
SERVICE_DATAMONKEY_PORT=9300

# HyPhy Configuration
# ==================
# Path to HyPhy executable (can be absolute or in PATH)
HYPHY_PATH=hyphy

# Base directory where HyPhy writes logs and results
HYPHY_BASE_PATH=/data/output

# JWT Authentication Configuration
# ================================
# JWT keys are used for both Slurm authentication and user tokens

# Path to the JWT key inside the container
# This is where the key will be placed in both Slurm and Datamonkey containers
JWT_KEY_PATH=/var/spool/slurm/statesave/jwt_hs256.key

# Volume mount for the JWT key (for Docker)
# Format: /path/to/local/key:/container/path:ro
JWT_KEY_VOLUME=./keys/jwt_hs256.key:/var/spool/slurm/statesave/jwt_hs256.key

# Note: To generate a compliant JWT key, run:
#   ./generate-jwt-key.sh
# This will create a key in the ./keys directory

# User Token Authentication
# -------------------------
# Enable or disable user token authentication for DELETE operations
USER_TOKEN_ENABLED=true

# Path to the JWT key for user tokens (optional)
# If not specified, will use the same key as Slurm (JWT_KEY_PATH)
# USER_JWT_KEY_PATH=/var/spool/datamonkey/jwt_hs256.key

# Conversation Tracker Configuration
# ==================================
# Controls how AI chat conversations are stored
# Currently only SQLite is supported
CONVERSATION_TRACKER_TYPE=SQLiteConversationTracker

# Path to SQLite database for conversation storage
CONVERSATION_DB_PATH=/data/stores/conversations.db

# AI Configuration
# ================
# Configuration for AI-powered chat interface

# AI provider to use
# Options: 'openai', 'google', 'anthropic'
MODEL_PROVIDER=google

# Model name (depends on provider)
# Examples: 'gpt-4-turbo', 'gemini-2.5-flash', 'claude-3-opus'
MODEL_NAME=gemini-2.5-flash

# Temperature controls randomness of responses (0.0-1.0)
# Lower = more deterministic, Higher = more creative
MODEL_TEMPERATURE=0.2

# API keys for the selected provider (uncomment the one you're using)
# OPENAI_API_KEY=your_api_key_here
# GOOGLE_API_KEY=your_api_key_here
# ANTHROPIC_API_KEY=your_api_key_here